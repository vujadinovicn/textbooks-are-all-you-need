{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use `jinaai/code_exercises` dataset for generating synthetic data. Firstly, we need to download and obtain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marko.DESKTOP-CS1PADQ\\Desktop\\necajetbrains\\venvv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Marko.DESKTOP-CS1PADQ\\.cache\\huggingface\\hub\\datasets--jinaai--code_exercises. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 1468146/1468146 [00:03<00:00, 442317.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"jinaai/code_exercises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will only work with smaller subset of data, since the original dataset has over 400k samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['problem', 'solution'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_subset = ds['train'].select(range(200))\n",
    "print(train_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will save the subset to json so we can process it easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 58.81ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset saved to train_subset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_subset.to_json('data/train_subset.json')\n",
    "print(f\"Subset saved to data/train_subset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to use some LLM for code translation. One of the best models for this task is `Salesforce/codet5-base`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"Salesforce/codet5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function for generating the prompt and code translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_python_to_kotlin(python_code):\n",
    "    input_text = f\"Translate the following Python code:\\n\\n{python_code} to Kotlin code:\\n\\n\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_length=150, num_beams=5, no_repeat_ngram_size=2)\n",
    "    kotlin_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return kotlin_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test code translation on some basic Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kotlin Code:\n",
      "  def( nums )()() )()())()()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "python_code_example = \"\"\"\n",
    "    total = 0\n",
    "    for num in nums:\n",
    "        if num > 10:\n",
    "            total += num\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "kotlin_translation = translate_python_to_kotlin(python_code_example)\n",
    "print(\"Kotlin Code:\\n\", kotlin_translation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the result of the function is not accurate at all. I've also tried many other Python code examples, but the results were similar. I've also experimented with some other models. That attempt was also unsuccessful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I couldn't find any appropriate model and method to use LLM for code translation, I have decided to do it manually by copying first 100 rows from `train_subset.json` file and pasting them with the well structured prompt to ChatGPT 3.5 (online). Finally, I have obtained translated code and stored it in `data/python_to_kotlin_data.jsonl` file. Now, we can fine tune our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
